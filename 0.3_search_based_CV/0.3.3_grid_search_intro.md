# Grid Search
- Grid Search of Hyperparameters
- Exhaustive search through a specified subset of hyperparameters of a learning algorithm.
- Examines all possible combinations of the specified hyperparameters.

# Grid Search of Hyperparameters
- Examines all possible combinations of the specified hyperparameters.
- Cartesian product
- Combinations: ℎ𝑦𝑝1 × ℎ𝑦𝑝2 × … × ℎ𝑦𝑝𝑛
    - Combinations: 3 × 3 = 9

# Grid Search - Limitations
-  Curse of dimensionality: possible combinations grow exponentially with the number of hyperparameters
- Computationally expensive
- Hyperparameter values are determined manually
- Not ideal for continuous hyperparameters
    - A subset of “reasonable” hyperparameter values are set manually
- Does not explore the entire hyperparameter space (not feasible)
- It performs worse than other searches (for models with complex hyperparameter spaces)

# Grid Search - Advantages
- For models with simpler hyperparameter spaces works well.
- It can be parallelized.

# Grid Search - Considerations
- Grid Search is the most expensive method in terms of total computation time. However, if run in parallel, it is fast in terms of wall clock time.
- Sometimes, we run a small grid, determine where the optimum lies, and then expand the grid in that direction