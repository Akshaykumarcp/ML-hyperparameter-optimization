# Hyperparameter Optimization
-  The process of finding the best Hyperparameters for a given dataset is called Hyperparameter Optimization or Hyperparameter Tuning.
- The best hyperparameters are those that maximize the performance of the machine learning algorithm.

# Hyperparameter Tuning: Search
A search consist of:
- Hyperparameter space (0.1 section of this repo)
- A method for sampling candidate hyperparameters (0.3 section of this repo)
- A cross-validation scheme (0.2 section of this repo)
- A performance metric to minimize (or maximize) 

# Hyperparameter Tuning: Challenges
- We canâ€™t define a formula to find the hyperparameters
- Try different combinations of hyperparameter and evaluate model performance
- The critical step is to choose how many different hyperparameter combinations we are going to test.

# Hyperparameter Tuning: Methods
- As the the number of hyperparameter combinations increases, the chance to get a better model and computational cost increases

# Hyperparameter Nature
- Some hyperparameters are discrete
    - number of estimators in ensemble models
- Some hyperparameters are continuous
    - Penalization coefficient
    - Number of samples per split
- Some hyperparameters are categorical:
    - Loss (deviance, exponential)
    - Regularization (Lasso, Ridge)

# Hyperparameter Tuning: Considerations
When we create hyperparameter sampling strategies we need to consider:
- Number of hyperparameters of the machine learning model
- The low effective dimension
- The nature of the parameters (discrete, continuous)
- The computing resources available to us

# Basic Hyperparameter Tuning Methods
- Manual Search (0.3.2 section of this repo)
- Grid Search (0.3.3 section of this repo)
- Random Search (0.3.4 section of this repo)