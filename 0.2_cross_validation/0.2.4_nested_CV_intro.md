# Nested Cross-Validation

# Model selection, CV & Generalization Error

When CV is performed multiple times with multiple models:
- Validation tests are the same for all models
- Validation set “leaks” information to model “selection” procedure
-  Generalization error is optimistically biased

- We need a different test set to get an unbiased evaluation of the generalization error of selected model (or selected hyperparameters for the purpose of this section)
- Particularly important for data science competitions.

# Nested CV
- ref link: Click [here](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/#:~:text=Nested%20cross%2Dvalidation%20is%20an,of%20overfitting%20the%20training%20dataset.&text=Typically%2C%20the%20k%2Dfold%20cross,model%20on%20the%20holdout%20fold.)

# Nested CV, considerations
- Computationally expensive
- Useful when we need a good estimation of the generalization error
- Different inner models may have different hyperparameters, although I would expect to be among the top performing hyperparameters

